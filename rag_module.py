import os
import re
import io
import csv
from datetime import datetime
import streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from supabase import create_client

# ==================== ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ë° ì—ëŸ¬ ì²˜ë¦¬ ====================
try:
    import pypdf
    import docx
    import openpyxl
    from pptx import Presentation  # PPT ì²˜ë¦¬ìš©
    from PIL import Image          # ì´ë¯¸ì§€ ì²˜ë¦¬ìš©
    import pytesseract             # OCRìš©
except ImportError as e:
    st.error(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    st.stop()

# ==================== ì„¹ì…˜/ì¡°í•­ ì¸ì‹ ë° ë¬¸ë§¥ ì£¼ì… í•¨ìˆ˜ ====================
def preprocess_text_with_section_headers(text):
    """
    ë¬¸ì„œ ë‚´ìš©ì„ ì¤„ ë‹¨ìœ„ë¡œ ì½ìœ¼ë©´ì„œ í—¤ë”(ì œNì¡°, ì œNì¥)ë¥¼ ê°ì§€í•˜ì—¬
    ì¼ë°˜ í…ìŠ¤íŠ¸ì—ë„ í•´ë‹¹ ì„¹ì…˜ ì •ë³´(Context)ë¥¼ ê°•ì œë¡œ ì£¼ì…í•©ë‹ˆë‹¤.
    """
    lines = text.split('\n')
    processed_lines = []
    
    current_section = "ì¼ë°˜"
    # ì •ê·œí‘œí˜„ì‹: ì œ1ì¡°, ì œ 1 ì¡°, ì œ1ì¥, 1. ê°€. ë“± (ì¡°/ì¥ ìœ„ì£¼ë¡œ ê°ì§€)
    header_pattern = re.compile(r'^\s*ì œ\s*\d+\s*(ì¡°|ì¥)')

    for line in lines:
        stripped_line = line.strip()
        if not stripped_line:
            continue
            
        # 1. ì´ë¯¸ ë¬¸ë§¥ íƒœê·¸([...])ê°€ ë¶™ì–´ìˆëŠ” ê²½ìš° (ì—‘ì…€/CSV ë“±)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        if stripped_line.startswith('[') and ']' in stripped_line:
            processed_lines.append(stripped_line)
            continue

        # 2. ê·œì •ì§‘ í—¤ë” ê°ì§€
        if header_pattern.match(stripped_line):
            current_section = stripped_line
            processed_lines.append(line)
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ì— ì„¹ì…˜ëª… ì£¼ì…
            enriched_line = f"[{current_section}] {stripped_line}"
            processed_lines.append(enriched_line)
            
    return "\n".join(processed_lines)

# ==================== íŒŒì¼ í¬ë§·ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ë“¤ ====================

def extract_text_from_pdf(file_stream):
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    text = ""
    try:
        pdf_reader = pypdf.PdfReader(file_stream)
        for page in pdf_reader.pages:
            extracted = page.extract_text()
            if extracted: text += extracted + "\n"
    except Exception as e: print(f"PDF Error: {e}")
    return text

def extract_text_from_docx(file_stream):
    """Word í…ìŠ¤íŠ¸ ë° í‘œ ì¶”ì¶œ"""
    text = ""
    try:
        doc = docx.Document(file_stream)
        for para in doc.paragraphs: text += para.text + "\n"
        for table in doc.tables:
            for row in table.rows:
                row_text = [cell.text.strip() for cell in row.cells]
                text += " | ".join(row_text) + "\n"
            text += "\n"
    except Exception as e: print(f"DOCX Error: {e}")
    return text

def extract_text_from_xlsx(file_stream, filename):
    """Excel í–‰ ë‹¨ìœ„ ë¬¸ë§¥í™” ì¶”ì¶œ"""
    text = ""
    try:
        wb = openpyxl.load_workbook(file_stream, data_only=True)
        for sheet_name in wb.sheetnames:
            sheet = wb[sheet_name]
            rows = list(sheet.rows)
            if not rows: continue
            
            # í—¤ë” ì¶”ì¶œ (ì²« ì¤„)
            headers = [str(cell.value).strip() if cell.value else f"ì—´{i}" for i, cell in enumerate(rows[0])]
            
            # ë°ì´í„° ì¶”ì¶œ
            for row in rows[1:]:
                row_parts = []
                for i, cell in enumerate(row):
                    if i < len(headers) and cell.value is not None:
                        val = str(cell.value).strip()
                        if val: row_parts.append(f"{headers[i]}: {val}")
                if row_parts:
                    text += f"[{filename}-{sheet_name}] " + ", ".join(row_parts) + "\n"
    except Exception as e: print(f"XLSX Error: {e}")
    return text

def extract_text_from_pptx(file_stream):
    """[ì‹ ê·œ] PowerPoint ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    text = ""
    try:
        prs = Presentation(file_stream)
        for i, slide in enumerate(prs.slides):
            slide_text = []
            # ìŠ¬ë¼ì´ë“œ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ ìƒì(Shape) ìˆœíšŒ
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    slide_text.append(shape.text)
            
            if slide_text:
                # ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸ë¥¼ ë¬¸ë§¥ìœ¼ë¡œ í¬í•¨
                page_content = "\n".join(slide_text)
                text += f"[ìŠ¬ë¼ì´ë“œ {i+1}í˜ì´ì§€] {page_content}\n"
    except Exception as e: print(f"PPTX Error: {e}")
    return text

def extract_text_from_txt(file_stream):
    """[ì‹ ê·œ] ì¼ë°˜ í…ìŠ¤íŠ¸ ë° ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì¶”ì¶œ"""
    try:
        # UTF-8 ì‹œë„ í›„ ì‹¤íŒ¨í•˜ë©´ CP949(í•œê¸€ ìœˆë„ìš°) ì‹œë„
        content = file_stream.read()
        try:
            return content.decode('utf-8')
        except UnicodeDecodeError:
            return content.decode('cp949')
    except Exception as e:
        print(f"TXT Error: {e}")
        return ""

def extract_text_from_csv(file_stream, filename):
    """[ì‹ ê·œ] CSV íŒŒì¼ ë¬¸ë§¥í™” ì¶”ì¶œ"""
    text = ""
    try:
        content = file_stream.read()
        try:
            decoded = content.decode('utf-8')
        except UnicodeDecodeError:
            decoded = content.decode('cp949')
            
        f = io.StringIO(decoded)
        reader = csv.reader(f)
        rows = list(reader)
        
        if not rows: return ""
        
        headers = rows[0]
        for row in rows[1:]:
            row_parts = []
            for i, val in enumerate(row):
                if i < len(headers) and val.strip():
                    row_parts.append(f"{headers[i]}: {val.strip()}")
            if row_parts:
                text += f"[{filename}] " + ", ".join(row_parts) + "\n"
    except Exception as e: print(f"CSV Error: {e}")
    return text

def extract_text_from_image(file_stream):
    """ì´ë¯¸ì§€ OCR ì¶”ì¶œ (í•œê¸€+ì˜ì–´)"""
    text = ""
    try:
        image = Image.open(file_stream)
        text = pytesseract.image_to_string(image, lang='kor+eng')
    except Exception as e: print(f"OCR Error: {e}")
    return text

# ==================== ë©”ì¸ ë¡œì§ ====================

def init_vector_store():
    supabase_url = os.environ.get("SUPABASE_URL")
    supabase_key = os.environ.get("SUPABASE_KEY")
    supabase_client = create_client(supabase_url, supabase_key)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    return {'supabase_client': supabase_client, 'embeddings': embeddings}

def sync_drive_to_db(folder_id, supabase_client):
    import google.auth
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaIoBaseDownload

    # 1. êµ¬ê¸€ ì¸ì¦ ë° íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    creds, _ = google.auth.default()
    service = build('drive', 'v3', credentials=creds)

    results = service.files().list(
        q=f"'{folder_id}' in parents and trashed = false",
        fields="files(id, name, mimeType)"
    ).execute()
    files = results.get('files', [])

    st.write(f"ğŸ” ì´ {len(files)}ê°œì˜ íŒŒì¼ ê°ì§€ë¨. ì²˜ë¦¬ ì‹œì‘...")

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = SupabaseVectorStore(
        client=supabase_client,
        embedding=embeddings,
        table_name="documents",
        query_name="match_documents"
    )

    success_count = 0
    progress_bar = st.progress(0)
    
    # ì§€ì› í™•ì¥ì ëª©ë¡
    SUPPORTED_EXTENSIONS = [
        'pdf', 'docx', 'xlsx', 'pptx', 'txt', 'csv', 'md', 
        'jpg', 'jpeg', 'png'
    ]
    
    for i, file in enumerate(files):
        file_id = file['id']
        file_name = file['name']
        ext = file_name.split('.')[-1].lower() if '.' in file_name else ""
        
        progress_bar.progress((i + 1) / len(files))
        
        if ext not in SUPPORTED_EXTENSIONS:
            st.warning(f"â© [Skip] ë¯¸ì§€ì› íŒŒì¼: {file_name}")
            continue

        try:
            # ë‹¤ìš´ë¡œë“œ
            request = service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while done is False:
                status, done = downloader.next_chunk()
            fh.seek(0)
            
            text_content = ""
            
            # [í™•ì¥ìë³„ ë¶„ê¸° ì²˜ë¦¬]
            if ext == 'pdf':
                text_content = extract_text_from_pdf(fh)
            elif ext == 'docx':
                text_content = extract_text_from_docx(fh)
            elif ext == 'xlsx':
                text_content = extract_text_from_xlsx(fh, file_name)
            elif ext == 'pptx':
                text_content = extract_text_from_pptx(fh)
            elif ext in ['txt', 'md']:
                text_content = extract_text_from_txt(fh)
            elif ext == 'csv':
                text_content = extract_text_from_csv(fh, file_name)
            elif ext in ['jpg', 'jpeg', 'png']:
                text_content = extract_text_from_image(fh)

            # ë‚´ìš© ê²€ì¦
            if not text_content or not text_content.strip():
                st.error(f"âš ï¸ [ë‚´ìš© ì—†ìŒ] {file_name} (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨)")
                continue

            # ì „ì²˜ë¦¬ (í—¤ë”/ë¬¸ë§¥ ì£¼ì…)
            enriched_text = preprocess_text_with_section_headers(text_content)
            
            # ì²­í¬ ë¶„í•  ë° ì €ì¥
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = text_splitter.split_text(enriched_text)
            
            docs = []
            for chunk in chunks:
                docs.append(Document(
                    page_content=chunk,
                    metadata={"source": file_name, "created_at": datetime.now().isoformat()}
                ))

            vector_store.add_documents(docs)
            st.success(f"âœ… [ì™„ë£Œ] {file_name}")
            success_count += 1
            
        except Exception as e:
            st.error(f"âŒ [ì—ëŸ¬] {file_name}: {e}")

    progress_bar.empty()
    return success_count

# ==================== ê²€ìƒ‰ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================

def search_similar_documents(query, supabase_client, embeddings, top_k=5):
    vector_store = SupabaseVectorStore(
        client=supabase_client,
        embedding=embeddings,
        table_name="documents",
        query_name="match_documents"
    )
    
    docs_with_score = vector_store.similarity_search_with_relevance_scores(query, k=top_k)
    
    filtered_docs = []
    filtered_infos = []
    
    for doc, score in docs_with_score:
        if score < 0.3: continue
        
        filtered_docs.append(doc)
        filtered_infos.append({
            "content": doc.page_content,
            "filename": doc.metadata.get("source", "Unknown"),
            "score": score
        })
        
    return filtered_docs, filtered_infos

def reset_database(supabase_client):
    try:
        supabase_client.table("documents").delete().neq("id", 0).execute()
        return True
    except:
        return False