import os
import re
import io
import csv
import hashlib
import time
from datetime import datetime
import streamlit as st

# Google Drive API import
import google.auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# Supabase Imports
from supabase import create_client, Client

# LangChain and Embedding Imports
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# File Parsing Libraries Imports
import pypdf
import docx
import openpyxl
from pptx import Presentation
from PIL import Image
import pytesseract

# ==================== [í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ë©”íƒ€ë°ì´í„° ë¶„ë¦¬ ê°œì„ ] ====================
def preprocess_text_with_section_headers(text):
    """
    ì„¹ì…˜ íƒœê·¸ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë¶„ë¦¬í•˜ì—¬ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ
    """
    if text:
        text = text.replace('\x00', '')

    lines = text.split('\n')
    chunks = []
    current_section = "ì¼ë°˜"
    header_pattern = re.compile(r'^\s*ì œ\s*\d+\s*(ì¡°|ì¥)')

    current_chunk_lines = []

    for line in lines:
        stripped_line = line.strip()
        if not stripped_line:
            continue

        # ì´ë¯¸ íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° (Excel, PPT ë“±)
        if stripped_line.startswith('[') and ']' in stripped_line:
            if current_chunk_lines:
                chunks.append({
                    "content": "\n".join(current_chunk_lines),
                    "section": current_section
                })
                current_chunk_lines = []
            # íƒœê·¸ ì œê±°í•˜ê³  ìˆœìˆ˜ ë‚´ìš©ë§Œ ì €ì¥
            content_only = re.sub(r'^\[.*?\]\s*', '', stripped_line)
            current_chunk_lines.append(content_only)
            continue

        if header_pattern.match(stripped_line):
            # ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘
            if current_chunk_lines:
                chunks.append({
                    "content": "\n".join(current_chunk_lines),
                    "section": current_section
                })
                current_chunk_lines = []
            current_section = stripped_line
            current_chunk_lines.append(stripped_line)
        else:
            # ìˆœìˆ˜ ë‚´ìš©ë§Œ ì €ì¥ (íƒœê·¸ ì—†ì´)
            current_chunk_lines.append(stripped_line)

    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk_lines:
        chunks.append({
            "content": "\n".join(current_chunk_lines),
            "section": current_section
        })

    return chunks

# ==================== [íŒŒì¼ í¬ë§·ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR ê°•í™”)] ====================
def extract_text_from_pdf(fh):
    text = ""
    try:
        reader = pypdf.PdfReader(fh)
        for page in reader.pages:
            t = page.extract_text()
            if t: text += t + "\n"
            else:
                try:
                    for image_file in page.images:
                        img = Image.open(io.BytesIO(image_file.data))
                        text += pytesseract.image_to_string(img, lang='kor+eng') + "\n"
                except: pass
    except Exception as e: print(f"PDF Error: {e}")
    return text

def extract_text_from_docx(fh):
    text = ""
    try:
        doc = docx.Document(fh)
        for para in doc.paragraphs: text += para.text + "\n"
        for table in doc.tables:
            for row in table.rows:
                row_text = [c.text.strip() for c in row.cells]
                text += " | ".join(row_text) + "\n"
            text += "\n"
    except Exception as e: print(f"DOCX Error: {e}")
    return text

def extract_text_from_xlsx(fh, fname):
    text = ""
    try:
        wb = openpyxl.load_workbook(fh, data_only=True)
        for sname in wb.sheetnames:
            sheet = wb[sname]
            rows = list(sheet.rows)
            if not rows: continue
            headers = [str(c.value).strip() if c.value else f"ì—´{i}" for i,c in enumerate(rows[0])]
            for row in rows[1:]:
                parts = []
                for i, c in enumerate(row):
                    if i < len(headers) and c.value is not None:
                        val = str(c.value).strip()
                        if val: parts.append(f"{headers[i]}: {val}")
                if parts: text += f"{fname}-{sname} " + ", ".join(parts) + "\n"
    except Exception as e: print(f"XLSX Error: {e}")
    return text

def extract_text_from_pptx(fh):
    text = ""
    try:
        prs = Presentation(fh)
        for i, slide in enumerate(prs.slides):
            stext = []
            for shape in slide.shapes:
                if hasattr(shape, "text"): stext.append(shape.text)
            if stext: text += f"ìŠ¬ë¼ì´ë“œ {i+1} " + "\n".join(stext) + "\n"
    except Exception as e: print(f"PPTX Error: {e}")
    return text

def extract_text_from_txt(fh):
    try:
        c = fh.read()
        try: return c.decode('utf-8')
        except: return c.decode('cp949')
    except: return ""

def extract_text_from_csv(fh, fname):
    text = ""
    try:
        c = fh.read()
        try: dec = c.decode('utf-8')
        except: dec = c.decode('cp949')
        rows = list(csv.reader(io.StringIO(dec)))
        if not rows: return ""
        headers = rows[0]
        for row in rows[1:]:
            parts = []
            for i, v in enumerate(row):
                if i < len(headers) and v.strip():
                    parts.append(f"{headers[i]}: {v.strip()}")
            if parts: text += f"{fname} " + ", ".join(parts) + "\n"
    except Exception as e: print(f"CSV Error: {e}")
    return text

def extract_text_from_image(fh):
    text = ""
    try:
        img = Image.open(fh)
        text = pytesseract.image_to_string(img, lang='kor+eng')
    except Exception as e: print(f"OCR Error: {e}")
    return text

# ==================== [íŒŒì¼ í¬ë§·ë³„ ìµœì  ì²­í¬ í¬ê¸°] ====================
def get_optimal_splitter(file_type):
    """íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ìµœì í™”ëœ ì²­í¬ í¬ê¸° ë°˜í™˜"""
    if file_type == 'xlsx':
        return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    elif file_type == 'pptx':
        return RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)
    elif file_type == 'csv':
        return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    else:
        return RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# ==================== [í•µì‹¬ ë¡œì§: Supabase ì—°ê²° ë° ë™ê¸°í™”] ====================

def init_vector_store():
    """
    Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")

    if not url or not key:
        raise ValueError("SUPABASE_URL ë˜ëŠ” SUPABASE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        supabase_client: Client = create_client(url, key)
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

        return {
            'supabase_client': supabase_client,
            'embeddings': embeddings
        }
    except Exception as e:
        print(f"Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def get_file_hash(content):
    """íŒŒì¼ ë‚´ìš©ì˜ í•´ì‹œê°’ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def delete_document_by_source(client, source_name):
    """
    íŠ¹ì • íŒŒì¼ëª…ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ DBì—ì„œ ì‚­ì œ
    ì¤‘ë³µ ë°©ì§€ ë° ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í•¨ìˆ˜
    """
    try:
        result = client.table("documents").delete().eq("metadata->>source", source_name).execute()
        print(f"âœ… {source_name} ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ {source_name} ì‚­ì œ ì‹¤íŒ¨: {e}")
        return False

def sync_drive_to_db(folder_id, supabase_client, force_update=False):
    """
    [ê°œì„ ëœ í•µì‹¬ í•¨ìˆ˜] Google Driveì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì™€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë²¡í„° DBì— ë™ê¸°í™”í•©ë‹ˆë‹¤.

    ê°œì„ ì‚¬í•­:
    - ì¤‘ë³µ ë°©ì§€: íŒŒì¼ë³„ë¡œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ì‚½ì…
    - íŒŒì¼ íƒ€ì…ë³„ ìµœì  ì²­í¬ í¬ê¸°
    - ì„¹ì…˜ íƒœê·¸ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë¶„ë¦¬
    - ì§„í–‰ìƒí™© ìƒì„¸ ë¡œê·¸

    Args:
        folder_id: Google Drive í´ë” ID
        supabase_client: Supabase í´ë¼ì´ì–¸íŠ¸
        force_update: Trueì¼ ê²½ìš° ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ í›„ ì¬ìƒ‰ì¸
    """
    creds, _ = google.auth.default()
    service = build('drive', 'v3', credentials=creds)

    res = service.files().list(
        q=f"'{folder_id}' in parents and trashed=false",
        fields="files(id, name, modifiedTime)"
    ).execute()
    files = res.get('files', [])

    st.write(f"ğŸ” í´ë” ë‚´ íŒŒì¼ {len(files)}ê°œ ê°ì§€ë¨")

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    vector_store = SupabaseVectorStore(
        client=supabase_client,
        embedding=embeddings,
        table_name="documents",
        query_name="match_documents"
    )

    cnt = 0
    skipped = 0
    failed = 0
    progress = st.progress(0)

    for i, f in enumerate(files):
        fid, fname = f['id'], f['name']
        ext = fname.split('.')[-1].lower() if '.' in fname else ""
        progress.progress((i+1)/len(files), text=f"ì²˜ë¦¬ ì¤‘: {fname}")

        if ext not in ['pdf', 'docx', 'xlsx', 'pptx', 'txt', 'csv', 'md', 'jpg', 'jpeg', 'png']:
            st.caption(f"â© [Skip] {fname} - ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹")
            skipped += 1
            continue

        try:
            # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ (ì¤‘ë³µ ë°©ì§€)
            if force_update:
                delete_document_by_source(supabase_client, fname)

            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            req = service.files().get_media(fileId=fid)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, req)
            done = False
            while not done:
                _, done = downloader.next_chunk()
            fh.seek(0)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content = ""
            if ext == 'pdf':
                content = extract_text_from_pdf(fh)
            elif ext == 'docx':
                content = extract_text_from_docx(fh)
            elif ext == 'xlsx':
                content = extract_text_from_xlsx(fh, fname)
            elif ext == 'pptx':
                content = extract_text_from_pptx(fh)
            elif ext in ['txt', 'md']:
                content = extract_text_from_txt(fh)
            elif ext == 'csv':
                content = extract_text_from_csv(fh, fname)
            elif ext in ['jpg', 'png', 'jpeg']:
                content = extract_text_from_image(fh)

            if not content.strip():
                st.warning(f"âš ï¸ {fname} - ë‚´ìš© ì—†ìŒ")
                skipped += 1
                continue

            # ì„¹ì…˜ ì¸ì‹ ì „ì²˜ë¦¬ (ë©”íƒ€ë°ì´í„° ë¶„ë¦¬)
            processed_chunks = preprocess_text_with_section_headers(content)

            # íŒŒì¼ íƒ€ì…ë³„ ìµœì  ì²­í¬ í¬ê¸°
            splitter = get_optimal_splitter(ext)

            # ë¬¸ì„œ ìƒì„± (ì„¹ì…˜ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ)
            docs = []
            for chunk_data in processed_chunks:
                # ì¶”ê°€ ì²­í¬ ë¶„í• 
                sub_chunks = splitter.split_text(chunk_data["content"])
                for sub_chunk in sub_chunks:
                    if sub_chunk.strip():
                        docs.append(Document(
                            page_content=sub_chunk,  # ìˆœìˆ˜ ë‚´ìš©ë§Œ
                            metadata={
                                "source": fname,
                                "section": chunk_data["section"],  # ì„¹ì…˜ì€ ë©”íƒ€ë°ì´í„°ì—
                                "file_type": ext,
                                "created_at": datetime.now().isoformat()
                            }
                        ))

            if docs:
                vector_store.add_documents(docs)
                st.success(f"âœ… {fname} ì™„ë£Œ ({len(docs)}ê°œ ì²­í¬)")
                cnt += 1
            else:
                st.warning(f"âš ï¸ {fname} - ìœ íš¨í•œ ì²­í¬ ì—†ìŒ")
                skipped += 1

        except Exception as e:
            st.error(f"âŒ {fname} ì‹¤íŒ¨: {str(e)[:100]}")
            print(f"ìƒì„¸ ì—ëŸ¬ - {fname}: {e}")
            failed += 1

    progress.empty()

    # ê²°ê³¼ ìš”ì•½
    st.info(f"""
    ğŸ“Š ë™ê¸°í™” ì™„ë£Œ
    - âœ… ì„±ê³µ: {cnt}ê°œ
    - â© ê±´ë„ˆëœ€: {skipped}ê°œ
    - âŒ ì‹¤íŒ¨: {failed}ê°œ
    - ğŸ“ ì „ì²´: {len(files)}ê°œ
    """)

    return cnt

def search_similar_documents_with_retry(query, client, embeddings, top_k=5, threshold=0.5, max_retries=3):
    """
    ì¬ì‹œë„ ë¡œì§ì´ ì¶”ê°€ëœ ê²€ìƒ‰ í•¨ìˆ˜
    """
    for attempt in range(max_retries):
        try:
            query_vector = embeddings.embed_query(query)

            params = {
                "query_embedding": query_vector,
                "match_threshold": threshold,
                "match_count": top_k
            }

            response = client.rpc("match_documents", params).execute()

            docs = []
            infos = []

            for item in response.data:
                content = item.get("content", "")
                metadata = item.get("metadata", {})
                score = item.get("similarity", 0.0)

                doc = Document(page_content=content, metadata=metadata)
                docs.append(doc)

                infos.append({
                    "content": content,
                    "filename": metadata.get("source", "Unknown"),
                    "section": metadata.get("section", "ì¼ë°˜"),
                    "score": score
                })

            return docs, infos

        except Exception as e:
            if attempt == max_retries - 1:
                print(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({max_retries}íšŒ ì‹œë„): {e}")
                return [], []
            print(f"ê²€ìƒ‰ ì¬ì‹œë„ {attempt + 1}/{max_retries}...")
            time.sleep(1 * (attempt + 1))

def search_similar_documents(query, client, embeddings, top_k=5, dynamic_threshold=True):
    """
    ë™ì  ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê°œì„ ëœ ê²€ìƒ‰ í•¨ìˆ˜

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        client: Supabase í´ë¼ì´ì–¸íŠ¸
        embeddings: ì„ë² ë”© ëª¨ë¸
        top_k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
        dynamic_threshold: Trueì¼ ê²½ìš° ë™ì  ì„ê³„ê°’ ì‚¬ìš©
    """
    if dynamic_threshold:
        # ë¨¼ì € ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ê²€ìƒ‰
        docs_high, infos_high = search_similar_documents_with_retry(
            query, client, embeddings, top_k=top_k, threshold=0.5
        )

        # ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ë°˜í™˜
        if len(docs_high) >= 3:
            return docs_high, infos_high

        # ê²°ê³¼ ë¶€ì¡± ì‹œ ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¬ê²€ìƒ‰
        docs_low, infos_low = search_similar_documents_with_retry(
            query, client, embeddings, top_k=top_k, threshold=0.15
        )
        return docs_low, infos_low
    else:
        # ê³ ì • ì„ê³„ê°’ ì‚¬ìš©
        return search_similar_documents_with_retry(
            query, client, embeddings, top_k=top_k, threshold=0.15
        )

def get_indexed_documents(client):
    """DBì— ìƒ‰ì¸ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        result = client.table("documents").select("metadata").execute()
        sources = set()
        for item in result.data:
            metadata = item.get("metadata", {})
            source = metadata.get("source", "Unknown")
            sources.add(source)
        return list(sources)
    except Exception as e:
        print(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

def reset_database(client):
    """
    Supabase DBì˜ documents í…Œì´ë¸” ì „ì²´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. (ê°•ì œ ì´ˆê¸°í™”)
    """
    if client is None:
        print("Error: Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False

    try:
        result = client.table("documents").delete().neq("id", 0).execute()

        if result.data == []:
             print("DB ì‚­ì œ ì„±ê³µ")
             return True
        else:
             print(f"DB ì‚­ì œ ì‹œë„ ì‘ë‹µì— ë°ì´í„°ê°€ ë‚¨ì•„ìˆìŒ: {result.data}")
             return False
    except Exception as e:
        print(f"DB Reset Error: {e}")
        return False
