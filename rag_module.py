import os
import re
import io
import csv
from datetime import datetime
import streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import SupabaseVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from supabase import create_client

# ==================== [ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì²´í¬] ====================
try:
    import pypdf
    import docx
    import openpyxl
    from pptx import Presentation
    from PIL import Image
    import pytesseract
except ImportError as e:
    raise ImportError(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶€ì¡±: {e}")

# ==================== [í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬] ====================
def preprocess_text_with_section_headers(text):
    if text: text = text.replace('\x00', '') # Null ë¬¸ì ì œê±°
    lines = text.split('\n')
    processed_lines = []
    current_section = "ì¼ë°˜"
    header_pattern = re.compile(r'^\s*ì œ\s*\d+\s*(ì¡°|ì¥)')

    for line in lines:
        stripped_line = line.strip()
        if not stripped_line: continue
        if stripped_line.startswith('[') and ']' in stripped_line:
            processed_lines.append(stripped_line)
            continue
        if header_pattern.match(stripped_line):
            current_section = stripped_line
            processed_lines.append(line)
        else:
            enriched_line = f"[{current_section}] {stripped_line}"
            processed_lines.append(enriched_line)
    return "\n".join(processed_lines)

# ==================== [íŒŒì¼ í¬ë§·ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ] ====================
def extract_text_from_pdf(fh):
    text = ""
    try:
        reader = pypdf.PdfReader(fh)
        for page in reader.pages:
            t = page.extract_text()
            if t: text += t + "\n"
            else:
                try:
                    for image_file in page.images:
                        img = Image.open(io.BytesIO(image_file.data))
                        text += pytesseract.image_to_string(img, lang='kor+eng') + "\n"
                except: pass
    except Exception as e: print(f"PDF Error: {e}")
    return text

def extract_text_from_docx(fh):
    text = ""
    try:
        doc = docx.Document(fh)
        for para in doc.paragraphs: text += para.text + "\n"
        for table in doc.tables:
            for row in table.rows:
                row_text = [c.text.strip() for c in row.cells]
                text += " | ".join(row_text) + "\n"
            text += "\n"
    except Exception as e: print(f"DOCX Error: {e}")
    return text

def extract_text_from_xlsx(fh, fname):
    text = ""
    try:
        wb = openpyxl.load_workbook(fh, data_only=True)
        for sname in wb.sheetnames:
            sheet = wb[sname]
            rows = list(sheet.rows)
            if not rows: continue
            headers = [str(c.value).strip() if c.value else f"ì—´{i}" for i,c in enumerate(rows[0])]
            for row in rows[1:]:
                parts = []
                for i, c in enumerate(row):
                    if i < len(headers) and c.value is not None:
                        val = str(c.value).strip()
                        if val: parts.append(f"{headers[i]}: {val}")
                if parts: text += f"[{fname}-{sname}] " + ", ".join(parts) + "\n"
    except Exception as e: print(f"XLSX Error: {e}")
    return text

def extract_text_from_pptx(fh):
    text = ""
    try:
        prs = Presentation(fh)
        for i, slide in enumerate(prs.slides):
            stext = []
            for shape in slide.shapes:
                if hasattr(shape, "text"): stext.append(shape.text)
            if stext: text += f"[ìŠ¬ë¼ì´ë“œ {i+1}] " + "\n".join(stext) + "\n"
    except Exception as e: print(f"PPTX Error: {e}")
    return text

def extract_text_from_txt(fh):
    try:
        c = fh.read()
        try: return c.decode('utf-8')
        except: return c.decode('cp949')
    except: return ""

def extract_text_from_csv(fh, fname):
    text = ""
    try:
        c = fh.read()
        try: dec = c.decode('utf-8')
        except: dec = c.decode('cp949')
        rows = list(csv.reader(io.StringIO(dec)))
        if not rows: return ""
        headers = rows[0]
        for row in rows[1:]:
            parts = []
            for i, v in enumerate(row):
                if i < len(headers) and v.strip():
                    parts.append(f"{headers[i]}: {v.strip()}")
            if parts: text += f"[{fname}] " + ", ".join(parts) + "\n"
    except Exception as e: print(f"CSV Error: {e}")
    return text

def extract_text_from_image(fh):
    text = ""
    try:
        img = Image.open(fh)
        text = pytesseract.image_to_string(img, lang='kor+eng')
    except Exception as e: print(f"OCR Error: {e}")
    return text

# ==================== [í•µì‹¬ ë¡œì§: Supabase ì—°ê²°] ====================

def init_vector_store():
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")
    return {
        'supabase_client': create_client(url, key),
        'embeddings': GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    }

def sync_drive_to_db(folder_id, supabase_client):
    import google.auth
    from googleapiclient.discovery import build
    from googleapiclient.http import MediaIoBaseDownload
    
    creds, _ = google.auth.default()
    service = build('drive', 'v3', credentials=creds)
    
    res = service.files().list(q=f"'{folder_id}' in parents and trashed=false", fields="files(id, name)").execute()
    files = res.get('files', [])
    
    st.write(f"ğŸ” í´ë” ë‚´ íŒŒì¼ {len(files)}ê°œ ê°ì§€ë¨")
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    # ì—…ë¡œë“œëŠ” LangChain ì‚¬ìš© (ë¬¸ì œ ì—†ìŒ)
    vector_store = SupabaseVectorStore(client=supabase_client, embedding=embeddings, table_name="documents", query_name="match_documents")
    
    cnt = 0
    progress = st.progress(0)
    
    for i, f in enumerate(files):
        fid, fname = f['id'], f['name']
        ext = fname.split('.')[-1].lower() if '.' in fname else ""
        progress.progress((i+1)/len(files))
        
        if ext not in ['pdf', 'docx', 'xlsx', 'pptx', 'txt', 'csv', 'md', 'jpg', 'jpeg', 'png']:
            st.warning(f"â© [Skip] {fname}")
            continue
            
        try:
            req = service.files().get_media(fileId=fid)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, req)
            done = False
            while not done: _, done = downloader.next_chunk()
            fh.seek(0)
            
            content = ""
            if ext == 'pdf': content = extract_text_from_pdf(fh)
            elif ext == 'docx': content = extract_text_from_docx(fh)
            elif ext == 'xlsx': content = extract_text_from_xlsx(fh, fname)
            elif ext == 'pptx': content = extract_text_from_pptx(fh)
            elif ext in ['txt', 'md']: content = extract_text_from_txt(fh)
            elif ext == 'csv': content = extract_text_from_csv(fh, fname)
            elif ext in ['jpg', 'png', 'jpeg']: content = extract_text_from_image(fh)
            
            if not content.strip():
                st.error(f"âš ï¸ {fname} ë‚´ìš© ì—†ìŒ")
                continue
                
            processed = preprocess_text_with_section_headers(content)
            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            chunks = splitter.split_text(processed)
            
            docs = [Document(page_content=c, metadata={"source": fname, "created_at": datetime.now().isoformat()}) for c in chunks]
            vector_store.add_documents(docs)
            st.success(f"âœ… {fname} ì™„ë£Œ")
            cnt += 1
        except Exception as e:
            st.error(f"âŒ {fname} ì‹¤íŒ¨: {e}")
            
    progress.empty()
    return cnt

# ==================== [í•µì‹¬ ìˆ˜ì •: ê²€ìƒ‰ í•¨ìˆ˜ (Native RPC í˜¸ì¶œ)] ====================
def search_similar_documents(query, client, embeddings, top_k=5):
    """
    LangChain wrapperë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  Supabase Clientë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    (SyncRPCFilterRequestBuilder ì—ëŸ¬ ë°©ì§€ìš©)
    """
    try:
        # 1. ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        query_vector = embeddings.embed_query(query)

        # 2. Supabase RPC ì§ì ‘ í˜¸ì¶œ
        params = {
            "query_embedding": query_vector,
            "match_threshold": 0.3, # ìœ ì‚¬ë„ ì„ê³„ê°’
            "match_count": top_k
        }
        
        # .execute()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ì‹¤í–‰
        response = client.rpc("match_documents", params).execute()
        
        # 3. ê²°ê³¼ ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
        docs = []
        infos = []
        
        # response.dataëŠ” ë¦¬ìŠ¤íŠ¸[ë”•ì…”ë„ˆë¦¬] í˜•íƒœ
        for item in response.data:
            content = item.get("content", "")
            metadata = item.get("metadata", {})
            score = item.get("similarity", 0.0)
            
            # Document ê°ì²´ ìƒì„±
            doc = Document(page_content=content, metadata=metadata)
            docs.append(doc)
            
            # ì •ë³´ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            infos.append({
                "content": content,
                "filename": metadata.get("source", "Unknown"),
                "score": score
            })
            
        return docs, infos

    except Exception as e:
        print(f"Search Error: {e}")
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ ì•±ì´ ë©ˆì¶”ì§€ ì•Šê²Œ í•¨
        return [], []

def get_indexed_documents(client):
    return []

def reset_database(client):
    try: 
        client.table("documents").delete().neq("id", 0).execute()
        return True
    except: 
        return False