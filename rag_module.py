import os
import re
import io
import csv
import hashlib
import time
from datetime import datetime
import streamlit as st

# Google Drive API import
import google.auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# Supabase Imports
from supabase import create_client, Client

# LangChain and Embedding Imports
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings

# File Parsing Libraries Imports
import pypdf
import docx
import openpyxl
from pptx import Presentation
from PIL import Image
import pytesseract

# ==================== [í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ë©”íƒ€ë°ì´í„° ë¶„ë¦¬ ê°œì„ ] ====================
def preprocess_text_with_section_headers(text):
    """
    ì„¹ì…˜ íƒœê·¸ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë¶„ë¦¬í•˜ì—¬ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ
    """
    if text:
        text = text.replace('\x00', '')

    lines = text.split('\n')
    chunks = []
    current_section = "ì¼ë°˜"
    header_pattern = re.compile(r'^\s*ì œ\s*\d+\s*(ì¡°|ì¥)')

    current_chunk_lines = []

    for line in lines:
        stripped_line = line.strip()
        if not stripped_line:
            continue

        # ì´ë¯¸ íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° (Excel, PPT ë“±)
        if stripped_line.startswith('[') and ']' in stripped_line:
            if current_chunk_lines:
                chunks.append({
                    "content": "\n".join(current_chunk_lines),
                    "section": current_section
                })
                current_chunk_lines = []
            # íƒœê·¸ ì œê±°í•˜ê³  ìˆœìˆ˜ ë‚´ìš©ë§Œ ì €ì¥
            content_only = re.sub(r'^\[.*?\]\s*', '', stripped_line)
            current_chunk_lines.append(content_only)
            continue

        if header_pattern.match(stripped_line):
            # ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘
            if current_chunk_lines:
                chunks.append({
                    "content": "\n".join(current_chunk_lines),
                    "section": current_section
                })
                current_chunk_lines = []
            current_section = stripped_line
            current_chunk_lines.append(stripped_line)
        else:
            # ìˆœìˆ˜ ë‚´ìš©ë§Œ ì €ì¥ (íƒœê·¸ ì—†ì´)
            current_chunk_lines.append(stripped_line)

    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk_lines:
        chunks.append({
            "content": "\n".join(current_chunk_lines),
            "section": current_section
        })

    return chunks

# ==================== [íŒŒì¼ í¬ë§·ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR ê°•í™”)] ====================
def extract_text_from_pdf(fh):
    text = ""
    try:
        reader = pypdf.PdfReader(fh)
        for page in reader.pages:
            t = page.extract_text()
            if t: text += t + "\n"
            else:
                try:
                    for image_file in page.images:
                        img = Image.open(io.BytesIO(image_file.data))
                        text += pytesseract.image_to_string(img, lang='kor+eng') + "\n"
                except: pass
    except Exception as e: print(f"PDF Error: {e}")
    return text

def extract_text_from_docx(fh):
    text = ""
    try:
        doc = docx.Document(fh)
        for para in doc.paragraphs: text += para.text + "\n"
        for table in doc.tables:
            for row in table.rows:
                row_text = [c.text.strip() for c in row.cells]
                text += " | ".join(row_text) + "\n"
            text += "\n"
    except Exception as e: print(f"DOCX Error: {e}")
    return text

def extract_text_from_xlsx(fh, fname):
    text = ""
    try:
        wb = openpyxl.load_workbook(fh, data_only=True)
        for sname in wb.sheetnames:
            sheet = wb[sname]
            rows = list(sheet.rows)
            if not rows: continue
            headers = [str(c.value).strip() if c.value else f"ì—´{i}" for i,c in enumerate(rows[0])]
            for row in rows[1:]:
                parts = []
                for i, c in enumerate(row):
                    if i < len(headers) and c.value is not None:
                        val = str(c.value).strip()
                        if val: parts.append(f"{headers[i]}: {val}")
                if parts: text += f"{fname}-{sname} " + ", ".join(parts) + "\n"
    except Exception as e: print(f"XLSX Error: {e}")
    return text

def extract_text_from_pptx(fh):
    text = ""
    try:
        prs = Presentation(fh)
        for i, slide in enumerate(prs.slides):
            stext = []
            for shape in slide.shapes:
                if hasattr(shape, "text"): stext.append(shape.text)
            if stext: text += f"ìŠ¬ë¼ì´ë“œ {i+1} " + "\n".join(stext) + "\n"
    except Exception as e: print(f"PPTX Error: {e}")
    return text

def extract_text_from_txt(fh):
    try:
        c = fh.read()
        try: return c.decode('utf-8')
        except: return c.decode('cp949')
    except: return ""

def extract_text_from_csv(fh, fname):
    text = ""
    try:
        c = fh.read()
        try: dec = c.decode('utf-8')
        except: dec = c.decode('cp949')
        rows = list(csv.reader(io.StringIO(dec)))
        if not rows: return ""
        headers = rows[0]
        for row in rows[1:]:
            parts = []
            for i, v in enumerate(row):
                if i < len(headers) and v.strip():
                    parts.append(f"{headers[i]}: {v.strip()}")
            if parts: text += f"{fname} " + ", ".join(parts) + "\n"
    except Exception as e: print(f"CSV Error: {e}")
    return text

def extract_text_from_image(fh):
    text = ""
    try:
        img = Image.open(fh)
        text = pytesseract.image_to_string(img, lang='kor+eng')
    except Exception as e: print(f"OCR Error: {e}")
    return text

# ==================== [íŒŒì¼ í¬ë§·ë³„ ìµœì  ì²­í¬ í¬ê¸°] ====================
def get_optimal_splitter(file_type):
    """
    íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ìµœì í™”ëœ ì²­í¬ í¬ê¸° ë°˜í™˜

    ê°œì„ : ì²­í¬ í¬ê¸° ëŒ€í­ ì¦ê°€ë¡œ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ
    - ë” ë§ì€ ë¬¸ë§¥ ì •ë³´ â†’ ë” ì •í™•í•œ ì˜ë¯¸ ì„ë² ë”©
    - ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
    """
    if file_type == 'xlsx':
        return RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)  # 500 â†’ 1500
    elif file_type == 'pptx':
        return RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)  # 1500 â†’ 2000
    elif file_type == 'csv':
        return RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)  # 500 â†’ 1500
    else:
        return RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=300)  # 1000 â†’ 2000

# ==================== [í•µì‹¬ ë¡œì§: Supabase ì—°ê²° ë° ë™ê¸°í™”] ====================

def init_vector_store():
    """
    Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_KEY")

    if not url or not key:
        raise ValueError("SUPABASE_URL ë˜ëŠ” SUPABASE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    try:
        supabase_client: Client = create_client(url, key)
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

        return {
            'supabase_client': supabase_client,
            'embeddings': embeddings
        }
    except Exception as e:
        print(f"Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise

def get_file_hash(content):
    """íŒŒì¼ ë‚´ìš©ì˜ í•´ì‹œê°’ ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def delete_document_by_source(client, source_name):
    """
    íŠ¹ì • íŒŒì¼ëª…ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ DBì—ì„œ ì‚­ì œ
    ì¤‘ë³µ ë°©ì§€ ë° ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í•¨ìˆ˜
    """
    try:
        result = client.table("documents").delete().eq("metadata->>source", source_name).execute()
        print(f"âœ… {source_name} ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ {source_name} ì‚­ì œ ì‹¤íŒ¨: {e}")
        return False

def get_file_timestamps_from_db(supabase_client):
    """
    DBì— ì €ì¥ëœ íŒŒì¼ë³„ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

    Returns:
        dict: {filename: last_modified_timestamp}
    """
    try:
        # documents í…Œì´ë¸”ì—ì„œ íŒŒì¼ë³„ ìµœì‹  last_modified ì¡°íšŒ
        result = supabase_client.table("documents").select("metadata").execute()

        file_times = {}
        for doc in result.data:
            metadata = doc.get('metadata', {})
            source = metadata.get('source')
            last_modified = metadata.get('last_modified')

            if source and last_modified:
                # ê°™ì€ íŒŒì¼ì˜ ì—¬ëŸ¬ ì²­í¬ ì¤‘ ê°€ì¥ ìµœì‹  ì‹œê°„ ìœ ì§€
                if source not in file_times or last_modified > file_times[source]:
                    file_times[source] = last_modified

        return file_times
    except Exception as e:
        print(f"DB íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {}

def sync_drive_to_db(folder_id, supabase_client, force_update=False):
    """
    [ê°œì„ ëœ í•µì‹¬ í•¨ìˆ˜] Google Driveì—ì„œ íŒŒì¼ì„ ê°€ì ¸ì™€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë²¡í„° DBì— ë™ê¸°í™”í•©ë‹ˆë‹¤.

    ê°œì„ ì‚¬í•­:
    - ğŸ†• ì¦ë¶„ ë™ê¸°í™”: ë³€ê²½ëœ íŒŒì¼ë§Œ ìë™ ê°ì§€í•˜ì—¬ ì—…ë°ì´íŠ¸
    - ì¤‘ë³µ ë°©ì§€: íŒŒì¼ë³„ë¡œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ì‚½ì…
    - íŒŒì¼ íƒ€ì…ë³„ ìµœì  ì²­í¬ í¬ê¸°
    - ì„¹ì…˜ íƒœê·¸ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ ë¶„ë¦¬
    - ì§„í–‰ìƒí™© ìƒì„¸ ë¡œê·¸

    Args:
        folder_id: Google Drive í´ë” ID
        supabase_client: Supabase í´ë¼ì´ì–¸íŠ¸
        force_update: Trueì¼ ê²½ìš° ì „ì²´ ì¬ìƒ‰ì¸ (ê¸°ë³¸ False = ì¦ë¶„ ë™ê¸°í™”)
    """
    creds, _ = google.auth.default()
    service = build('drive', 'v3', credentials=creds)

    # Driveì—ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (modifiedTime í¬í•¨)
    res = service.files().list(
        q=f"'{folder_id}' in parents and trashed=false",
        fields="files(id, name, modifiedTime)"
    ).execute()
    files = res.get('files', [])

    st.write(f"ğŸ” í´ë” ë‚´ íŒŒì¼ {len(files)}ê°œ ê°ì§€ë¨")

    # ì¦ë¶„ ë™ê¸°í™”: ë³€ê²½ëœ íŒŒì¼ë§Œ ê°ì§€
    files_to_process = []
    files_to_delete = []

    if not force_update:
        # DBì—ì„œ ê¸°ì¡´ íŒŒì¼ì˜ last_modified ì¡°íšŒ
        db_file_times = get_file_timestamps_from_db(supabase_client)
        drive_file_names = {f['name'] for f in files}

        new_count = 0
        updated_count = 0
        unchanged_count = 0

        for f in files:
            fname = f['name']
            drive_modified = f.get('modifiedTime', '')

            if fname not in db_file_times:
                # ìƒˆ íŒŒì¼
                files_to_process.append(f)
                new_count += 1
            elif drive_modified > db_file_times[fname]:
                # ìˆ˜ì •ëœ íŒŒì¼
                files_to_process.append(f)
                updated_count += 1
            else:
                # ë³€ê²½ ì—†ìŒ
                unchanged_count += 1

        # Driveì— ì—†ì§€ë§Œ DBì— ìˆëŠ” íŒŒì¼ = ì‚­ì œëœ íŒŒì¼
        for fname in db_file_times:
            if fname not in drive_file_names:
                files_to_delete.append(fname)

        # ë³€ê²½ ì‚¬í•­ ìš”ì•½
        st.info(f"""
        ğŸ“Š ì¦ë¶„ ë™ê¸°í™” ë¶„ì„
        - ğŸ†• ìƒˆ íŒŒì¼: {new_count}ê°œ
        - ğŸ”„ ìˆ˜ì •ëœ íŒŒì¼: {updated_count}ê°œ
        - âœ… ë³€ê²½ ì—†ìŒ: {unchanged_count}ê°œ
        - ğŸ—‘ï¸ ì‚­ì œëœ íŒŒì¼: {len(files_to_delete)}ê°œ
        """)

        if len(files_to_process) == 0 and len(files_to_delete) == 0:
            st.success("âœ¨ ëª¨ë“  ë¬¸ì„œê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤!")
            return 0
    else:
        # ì „ì²´ ì¬ìƒ‰ì¸ ëª¨ë“œ
        st.info("ğŸ”„ ì „ì²´ ì¬ìƒ‰ì¸ ëª¨ë“œ (ëª¨ë“  íŒŒì¼ ì²˜ë¦¬)")
        files_to_process = files

    # ì‚­ì œëœ íŒŒì¼ ì²˜ë¦¬
    deleted_count = 0
    if files_to_delete:
        st.write("ğŸ—‘ï¸ ì‚­ì œëœ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        for fname in files_to_delete:
            if delete_document_by_source(supabase_client, fname):
                deleted_count += 1
                st.caption(f"  âœ… {fname} ì œê±°ë¨")

    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

    cnt = 0
    skipped = 0
    failed = 0
    total_to_process = len(files_to_process)

    if total_to_process == 0:
        st.success(f"âœ… ë™ê¸°í™” ì™„ë£Œ (ì‚­ì œ: {deleted_count}ê°œ)")
        return deleted_count

    progress = st.progress(0)

    for i, f in enumerate(files_to_process):
        fid, fname = f['id'], f['name']
        drive_modified = f.get('modifiedTime', '')
        ext = fname.split('.')[-1].lower() if '.' in fname else ""
        progress.progress((i+1)/total_to_process, text=f"ì²˜ë¦¬ ì¤‘: {fname}")

        if ext not in ['pdf', 'docx', 'xlsx', 'pptx', 'txt', 'csv', 'md', 'jpg', 'jpeg', 'png']:
            st.caption(f"â© [Skip] {fname} - ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹")
            skipped += 1
            continue

        try:
            # ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ (ì¦ë¶„ ë™ê¸°í™” ë˜ëŠ” ì „ì²´ ì¬ìƒ‰ì¸)
            # ì¦ë¶„ ëª¨ë“œì—ì„œëŠ” ìˆ˜ì •/ìƒˆ íŒŒì¼ë§Œ ì—¬ê¸° ë„ë‹¬í•˜ë¯€ë¡œ í•­ìƒ ì‚­ì œ
            delete_document_by_source(supabase_client, fname)

            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            req = service.files().get_media(fileId=fid)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, req)
            done = False
            while not done:
                _, done = downloader.next_chunk()
            fh.seek(0)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content = ""
            if ext == 'pdf':
                content = extract_text_from_pdf(fh)
            elif ext == 'docx':
                content = extract_text_from_docx(fh)
            elif ext == 'xlsx':
                content = extract_text_from_xlsx(fh, fname)
            elif ext == 'pptx':
                content = extract_text_from_pptx(fh)
            elif ext in ['txt', 'md']:
                content = extract_text_from_txt(fh)
            elif ext == 'csv':
                content = extract_text_from_csv(fh, fname)
            elif ext in ['jpg', 'png', 'jpeg']:
                content = extract_text_from_image(fh)

            if not content.strip():
                st.warning(f"âš ï¸ {fname} - ë‚´ìš© ì—†ìŒ")
                skipped += 1
                continue

            # ì„¹ì…˜ ì¸ì‹ ì „ì²˜ë¦¬ (ë©”íƒ€ë°ì´í„° ë¶„ë¦¬)
            processed_chunks = preprocess_text_with_section_headers(content)

            # íŒŒì¼ íƒ€ì…ë³„ ìµœì  ì²­í¬ í¬ê¸°
            splitter = get_optimal_splitter(ext)

            # ë¬¸ì„œ ìƒì„± (ì„¹ì…˜ ì •ë³´ë¥¼ ë©”íƒ€ë°ì´í„° + ë³¸ë¬¸ì— í¬í•¨)
            docs = []
            for chunk_data in processed_chunks:
                # ì¶”ê°€ ì²­í¬ ë¶„í• 
                sub_chunks = splitter.split_text(chunk_data["content"])
                for sub_chunk in sub_chunks:
                    if sub_chunk.strip():
                        section_name = chunk_data["section"]

                        # ğŸ”§ ê°œì„ : ì„¹ì…˜ ì •ë³´ë¥¼ page_contentì—ë„ í¬í•¨í•˜ì—¬ ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ
                        # "ì¼ë°˜" ì„¹ì…˜ì´ ì•„ë‹ˆë©´ ì„¹ì…˜ëª…ì„ ë³¸ë¬¸ ì•ì— ì¶”ê°€
                        if section_name and section_name != "ì¼ë°˜":
                            enhanced_content = f"[{section_name}] {sub_chunk}"
                        else:
                            enhanced_content = sub_chunk

                        docs.append(Document(
                            page_content=enhanced_content,  # ì„¹ì…˜ ì •ë³´ í¬í•¨
                            metadata={
                                "source": fname,
                                "section": section_name,  # ë©”íƒ€ë°ì´í„°ì—ë„ ìœ ì§€
                                "file_type": ext,
                                "last_modified": drive_modified,
                                "created_at": datetime.now().isoformat()
                            }
                        ))

            if docs:
                # ğŸ”§ ê°œì„ : SupabaseVectorStore ëŒ€ì‹  ì§ì ‘ ì €ì¥ (ì„ë² ë”© ì°¨ì› ì˜¤ë¥˜ í•´ê²°)
                try:
                    for doc in docs:
                        # ì„ë² ë”© ìƒì„± (768ì°¨ì›)
                        embedding_vector = embeddings.embed_query(doc.page_content)

                        # Supabaseì— ì§ì ‘ ì‚½ì…
                        supabase_client.table("documents").insert({
                            "content": doc.page_content,
                            "metadata": doc.metadata,
                            "embedding": embedding_vector  # 768ì°¨ì› ë¦¬ìŠ¤íŠ¸
                        }).execute()

                    st.success(f"âœ… {fname} ì™„ë£Œ ({len(docs)}ê°œ ì²­í¬)")
                    cnt += 1
                except Exception as insert_error:
                    st.error(f"âŒ {fname} ì‚½ì… ì‹¤íŒ¨: {str(insert_error)[:100]}")
                    print(f"ì‚½ì… ì—ëŸ¬ - {fname}: {insert_error}")
                    failed += 1
            else:
                st.warning(f"âš ï¸ {fname} - ìœ íš¨í•œ ì²­í¬ ì—†ìŒ")
                skipped += 1

        except Exception as e:
            st.error(f"âŒ {fname} ì‹¤íŒ¨: {str(e)[:100]}")
            print(f"ìƒì„¸ ì—ëŸ¬ - {fname}: {e}")
            failed += 1

    progress.empty()

    # ê²°ê³¼ ìš”ì•½
    if not force_update:
        st.info(f"""
        ğŸ“Š ì¦ë¶„ ë™ê¸°í™” ì™„ë£Œ
        - âœ… ìƒ‰ì¸ ì„±ê³µ: {cnt}ê°œ
        - ğŸ—‘ï¸ ì‚­ì œ ì²˜ë¦¬: {deleted_count}ê°œ
        - â© ê±´ë„ˆëœ€: {skipped}ê°œ
        - âŒ ì‹¤íŒ¨: {failed}ê°œ
        """)
    else:
        st.info(f"""
        ğŸ“Š ì „ì²´ ì¬ìƒ‰ì¸ ì™„ë£Œ
        - âœ… ì„±ê³µ: {cnt}ê°œ
        - â© ê±´ë„ˆëœ€: {skipped}ê°œ
        - âŒ ì‹¤íŒ¨: {failed}ê°œ
        - ğŸ“ ì „ì²´: {len(files)}ê°œ
        """)

    return cnt + deleted_count

def search_similar_documents_with_retry(query, client, embeddings, top_k=5, threshold=0.5, max_retries=3):
    """
    ì¬ì‹œë„ ë¡œì§ì´ ì¶”ê°€ëœ ê²€ìƒ‰ í•¨ìˆ˜
    """
    for attempt in range(max_retries):
        try:
            query_vector = embeddings.embed_query(query)

            params = {
                "query_embedding": query_vector,
                "match_threshold": threshold,
                "match_count": top_k
            }

            response = client.rpc("match_documents", params).execute()

            docs = []
            infos = []

            for item in response.data:
                content = item.get("content", "")
                metadata = item.get("metadata", {})
                score = item.get("similarity", 0.0)

                doc = Document(page_content=content, metadata=metadata)
                docs.append(doc)

                infos.append({
                    "content": content,
                    "filename": metadata.get("source", "Unknown"),
                    "section": metadata.get("section", "ì¼ë°˜"),
                    "score": score
                })

            return docs, infos

        except Exception as e:
            if attempt == max_retries - 1:
                print(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({max_retries}íšŒ ì‹œë„): {e}")
                return [], []
            print(f"ê²€ìƒ‰ ì¬ì‹œë„ {attempt + 1}/{max_retries}...")
            time.sleep(1 * (attempt + 1))

def search_similar_documents(query, client, embeddings, top_k=5, dynamic_threshold=True):
    """
    ë™ì  ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê°œì„ ëœ ê²€ìƒ‰ í•¨ìˆ˜

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        client: Supabase í´ë¼ì´ì–¸íŠ¸
        embeddings: ì„ë² ë”© ëª¨ë¸
        top_k: ë°˜í™˜í•  ë¬¸ì„œ ê°œìˆ˜
        dynamic_threshold: Trueì¼ ê²½ìš° ë™ì  ì„ê³„ê°’ ì‚¬ìš©

    ê°œì„ ì‚¬í•­:
        - ì„ê³„ê°’ ëŒ€í­ ë‚®ì¶¤: 0.5 â†’ 0.3, 0.15 â†’ 0.1
        - ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ê°€ëŠ¥
    """
    if dynamic_threshold:
        # ë¨¼ì € ì¤‘ê°„ ì„ê³„ê°’ìœ¼ë¡œ ê²€ìƒ‰ (0.5 â†’ 0.3)
        docs_high, infos_high = search_similar_documents_with_retry(
            query, client, embeddings, top_k=top_k, threshold=0.3  # ğŸ”§ 0.5 â†’ 0.3
        )

        # ê²°ê³¼ê°€ ì¶©ë¶„í•˜ë©´ ë°˜í™˜ (3ê°œ â†’ 5ê°œë¡œ ìƒí–¥)
        if len(docs_high) >= 5:  # ğŸ”§ 3 â†’ 5
            return docs_high, infos_high

        # ê²°ê³¼ ë¶€ì¡± ì‹œ ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¬ê²€ìƒ‰ (0.15 â†’ 0.1)
        docs_low, infos_low = search_similar_documents_with_retry(
            query, client, embeddings, top_k=top_k, threshold=0.1  # ğŸ”§ 0.15 â†’ 0.1
        )
        return docs_low, infos_low
    else:
        # ê³ ì • ì„ê³„ê°’ ì‚¬ìš© (0.15 â†’ 0.1)
        return search_similar_documents_with_retry(
            query, client, embeddings, top_k=top_k, threshold=0.1  # ğŸ”§ 0.15 â†’ 0.1
        )

def get_indexed_documents(client):
    """DBì— ìƒ‰ì¸ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        result = client.table("documents").select("metadata").execute()
        sources = set()
        for item in result.data:
            metadata = item.get("metadata", {})
            source = metadata.get("source", "Unknown")
            sources.add(source)
        return list(sources)
    except Exception as e:
        print(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return []

def reset_database(client):
    """
    Supabase DBì˜ documents í…Œì´ë¸” ì „ì²´ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. (ê°•ì œ ì´ˆê¸°í™”)

    ê°œì„ ëœ ì‚­ì œ ë°©ë²•:
    1. ë¨¼ì € ì „ì²´ ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
    2. ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì‚­ì œ (Supabase ì œí•œ ê³ ë ¤)
    3. ì‚­ì œ í›„ ì¬í™•ì¸
    """
    if client is None:
        print("Error: Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False

    try:
        # 1. ì‚­ì œ ì „ ë¬¸ì„œ ê°œìˆ˜ í™•ì¸
        count_before = client.table("documents").select("id", count="exact").execute()
        total_docs = count_before.count if hasattr(count_before, 'count') else 0
        print(f"ì‚­ì œ ì „ ë¬¸ì„œ ê°œìˆ˜: {total_docs}")

        if total_docs == 0:
            print("ì´ë¯¸ ë¹ˆ DBì…ë‹ˆë‹¤.")
            return True

        # 2. ì „ì²´ ì‚­ì œ ì‹¤í–‰ (neq ì¡°ê±´ìœ¼ë¡œ ëª¨ë“  í–‰ ì‚­ì œ)
        # SupabaseëŠ” idê°€ UUIDì´ë¯€ë¡œ gte('id', '00000000-0000-0000-0000-000000000000') ì‚¬ìš©
        result = client.table("documents").delete().gte("id", "00000000-0000-0000-0000-000000000000").execute()

        print(f"ì‚­ì œ ì‹¤í–‰ ì™„ë£Œ. ì‘ë‹µ: {result}")

        # 3. ì‚­ì œ í›„ ì¬í™•ì¸
        count_after = client.table("documents").select("id", count="exact").execute()
        remaining_docs = count_after.count if hasattr(count_after, 'count') else 0
        print(f"ì‚­ì œ í›„ ë‚¨ì€ ë¬¸ì„œ: {remaining_docs}")

        if remaining_docs == 0:
            print(f"âœ… DB ì‚­ì œ ì„±ê³µ: {total_docs}ê°œ ë¬¸ì„œ ì‚­ì œë¨")
            return True
        else:
            print(f"âš ï¸ ì™„ì „ ì‚­ì œ ì‹¤íŒ¨: {remaining_docs}ê°œ ë¬¸ì„œ ë‚¨ìŒ")
            return False

    except Exception as e:
        print(f"DB Reset Error: {e}")
        import traceback
        traceback.print_exc()
        return False
