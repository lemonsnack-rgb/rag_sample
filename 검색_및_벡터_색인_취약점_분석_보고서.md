# 🔍 RAG 시스템 검색 및 벡터 색인 취약점 분석 보고서

**분석 일시:** 2025-12-14
**분석 대상:** WorkAnswer RAG 시스템 (app.py, rag_module.py, Supabase DB)
**현재 상태:** "문서에 있는 것이 제대로 검색되지 않는 문제" 발생 중

---

## 📋 Executive Summary

**주요 발견:**
- 🔴 **치명적 취약점 1개** - 문서 중복 색인 (DB 삭제 필요성의 근본 원인)
- 🟠 **심각한 취약점 3개** - 검색 임계값, 쿼리 확장 노이즈, 섹션 태그 부작용
- 🟡 **개선 필요 4개** - 중복 제거 로직, 메타데이터 처리, 청크 크기, 에러 핸들링

**DB 삭제가 필요한 이유:**
현재 시스템은 **중복 방지 메커니즘이 전혀 없어서**, 같은 파일을 여러 번 동기화하면 DB에 중복 색인됨. 이미 중복이 발생했다면 **선택적 삭제 기능이 없으므로 전체 DB 삭제만이 유일한 해결책**.

---

## 🚨 치명적 취약점 (Critical)

### 1. **문서 중복 색인 문제** - DB 삭제 필요성의 근본 원인

#### 📍 위치: `rag_module.py:166-230` - `sync_drive_to_db()`

#### 문제점:
```python
def sync_drive_to_db(folder_id, supabase_client):
    # ❌ 기존 문서 확인 없음
    # ❌ 파일 해시나 버전 관리 없음
    # ❌ 업데이트 로직 없음 (삭제 후 재삽입 X, 무조건 추가만)

    for i, f in enumerate(files):
        # ... 파일 다운로드 및 텍스트 추출 ...

        docs = [Document(page_content=c, metadata={"source": fname, ...}) for c in chunks]
        vector_store.add_documents(docs)  # ⚠️ 항상 추가만 수행
```

#### 증상:
1. **시나리오 1: 반복 동기화**
   - 사용자가 "문서 동기화" 버튼을 2번 클릭
   - 같은 파일 24개 × 2 = 48개 문서 색인
   - 검색 시 같은 내용이 2번씩 반환

2. **시나리오 2: 파일 수정 후 재동기화**
   - "규정.pdf" 파일 수정 후 Drive에 업로드
   - "문서 동기화" 다시 클릭
   - 구버전 + 신버전 동시 존재
   - 검색 시 구버전이 높은 점수로 반환될 수 있음

3. **시나리오 3: 파일명 변경**
   - "인건비규정.pdf" → "인건비_규정_v2.pdf"로 변경
   - 재동기화 시 두 파일 모두 색인 (실제로는 같은 내용)

#### 검증 방법:
```sql
-- Supabase SQL Editor에서 실행
-- 1. 같은 source가 몇 개나 있는지 확인
SELECT
    metadata->>'source' as filename,
    COUNT(*) as chunk_count
FROM documents
GROUP BY metadata->>'source'
ORDER BY chunk_count DESC;

-- 2. 완전히 동일한 content 찾기
SELECT
    content,
    metadata->>'source' as filename,
    COUNT(*) as duplicate_count
FROM documents
GROUP BY content, metadata->>'source'
HAVING COUNT(*) > 1
ORDER BY duplicate_count DESC
LIMIT 10;

-- 3. 같은 파일명에서 created_at이 다른 경우 (반복 색인 증거)
SELECT
    metadata->>'source' as filename,
    COUNT(DISTINCT DATE(created_at)) as sync_dates,
    COUNT(*) as total_chunks
FROM documents
GROUP BY metadata->>'source'
HAVING COUNT(DISTINCT DATE(created_at)) > 1
ORDER BY sync_dates DESC;
```

#### 왜 DB 삭제만이 유일한 해결책인가?

**현재 코드의 한계:**
- ✅ `reset_database()` - 전체 삭제만 가능
- ❌ 파일별 선택 삭제 함수 없음
- ❌ 중복 제거 함수 없음
- ❌ 증분 업데이트 로직 없음

**수동 SQL 삭제의 어려움:**
- 파일명이 다양한 인코딩으로 저장될 수 있음 (UTF-8, CP949)
- metadata가 JSONB 타입이라 복잡한 쿼리 필요
- 여러 파일을 선택적으로 삭제하려면 각각 SQL 실행 필요

**결론:**
중복이 이미 발생했다면 **DB 전체 삭제 후 재색인이 가장 빠르고 확실한 방법**.

---

## 🟠 심각한 취약점 (High)

### 2. **검색 임계값 0.15의 양날의 검** - 노이즈 증가 vs 재현율 향상

#### 📍 위치: `rag_module.py:241`

```python
params = {
    "query_embedding": query_vector,
    "match_threshold": 0.15, # 🌟🌟🌟 임계값 0.15로 재조정 🌟🌟🌟
    "match_count": top_k
}
```

#### 문제점:

**코사인 유사도 점수 의미:**
- 1.0 = 완벽히 동일
- 0.8-1.0 = 매우 관련 있음
- 0.5-0.8 = 관련 있음
- 0.3-0.5 = 약간 관련
- **0.15-0.3 = 거의 관련 없음**
- 0.0-0.15 = 완전 무관

**0.15 임계값의 영향:**
- ✅ 장점: 검색 누락 최소화 (높은 재현율)
- ❌ 단점: **관련 없는 문서도 대량 반환** (낮은 정밀도)

**실제 데이터로 확인 필요:**
```python
# 검색 결과의 점수 분포 확인
query = "인건비 지급 규정"
docs, infos = search_similar_documents(query, client, embeddings, top_k=20)
for i, info in enumerate(infos):
    print(f"{i+1}. {info['filename']} - Score: {info['score']:.3f}")

# 예상 결과:
# 1. 인건비규정.pdf - Score: 0.850  ✅ 관련 있음
# 2. 급여지급.pdf - Score: 0.720    ✅ 관련 있음
# 3. 회계규정.pdf - Score: 0.450    ⚠️ 약간 관련
# 4. 출장비.pdf - Score: 0.280      ❌ 거의 무관
# 5. 사무실운영.pdf - Score: 0.170  ❌ 완전 무관
```

#### 권장 사항:
```python
# 동적 임계값 사용
def search_with_dynamic_threshold(query, client, embeddings):
    # 먼저 높은 임계값으로 검색
    docs_high, infos_high = search_similar_documents(query, client, embeddings, top_k=5)

    # 결과가 5개 미만이면 임계값 낮춰서 재검색
    if len(docs_high) < 3:
        docs_low, infos_low = search_similar_documents_with_threshold(query, client, embeddings, threshold=0.15)
        return docs_low, infos_low
    else:
        return docs_high, infos_high
```

---

### 3. **양방향 쿼리 확장의 노이즈 폭발** - 검색 품질 저하의 주범

#### 📍 위치: `app.py:131-148`

```python
def expand_query(original_query, llm):
    final = [original_query]

    for k, v in st.session_state.dynamic_synonyms.items():
        if k in original_query:  # ⚠️ 부분 문자열 매칭
            final.extend(v)
        elif any(word in original_query for word in v):  # ⚠️ 역방향도 부분 매칭
             final.append(k)
```

#### 문제 시나리오:

**사전 내용:**
```python
{
    "심사료": ["게재료", "투고료", "논문 게재", "학회비", "논문 심사료"],
    "인건비": ["노무비", "인력운영비", "학생 인건비"]
}
```

**사용자 쿼리:** "학생 인건비 지급 기준이 뭐야?"

**확장 과정:**
1. "인건비" in "학생 인건비 지급 기준이 뭐야?" → True
   - 추가: ["노무비", "인력운영비", "학생 인건비"]
2. "학생 인건비" in "학생 인건비 지급 기준이 뭐야?" → True
   - 추가: "인건비" (이미 있음, 중복)
3. LLM 확장: ["지급", "기준"]

**최종 검색 키워드:**
```python
["학생 인건비 지급 기준이 뭐야?", "노무비", "인력운영비", "학생 인건비", "인건비", "지급", "기준"]
# 총 7개 키워드
```

**문제점:**
- 각 키워드로 top_k=5개씩 검색 → 최대 35개 문서 반환
- "노무비"는 사용자 의도와 다를 수 있음 (건설업 노무비 vs 연구비 인건비)
- "지급", "기준" 같은 일반 단어는 모든 규정 문서에 등장

#### 실제 영향:

```python
# app.py:283-290
for q in search_queries:  # 7개 키워드
    docs, infos = search_similar_documents(q, ...)  # 각각 5개 반환
    # 중복 제거는 완전 동일한 page_content만 제거
    # 실제로는 같은 문서의 다른 청크들도 모두 포함됨
```

**결과:**
- 15개 문서 중 실제 관련 문서 5개, 노이즈 10개
- 프롬프트에 관련 없는 내용 과다 포함
- LLM이 잘못된 문서 참조하여 답변
- 사용자: "문서에 있는데 검색 안됨" 호소

#### 개선 방안:

**1. 완전 단어 매칭 사용:**
```python
import re

def expand_query_v2(original_query, llm):
    final = [original_query]

    for k, v in st.session_state.dynamic_synonyms.items():
        # 완전 단어 경계 매칭
        if re.search(rf'\b{re.escape(k)}\b', original_query):
            final.extend(v)
        elif any(re.search(rf'\b{re.escape(word)}\b', original_query) for word in v):
            final.append(k)

    # 중복 제거 및 원본 쿼리 우선
    return [original_query] + list(set(final[1:]))
```

**2. 동의어 우선순위:**
```python
DEFAULT_SYNONYMS = {
    "심사료": {
        "primary": ["논문 심사료", "논문 게재"],  # 높은 우선순위
        "secondary": ["게재료", "투고료", "학회비"]  # 낮은 우선순위
    }
}
```

---

### 4. **섹션 태그 임베딩의 부작용** - 의도치 않은 검색 매칭

#### 📍 위치: `rag_module.py:20-44`

```python
def preprocess_text_with_section_headers(text):
    # ...
    enriched_line = f"[{current_section}] {stripped_line}"
    # 예: "[제1조] 본 규정은 ..."
```

#### 문제점:

**임베딩되는 실제 텍스트:**
```
[제1조] 본 규정은 인건비 지급에 관한 사항을 정함을 목적으로 한다.
[제1조] 인건비는 월급제를 원칙으로 한다.
[제2조] 학생 인건비는 시간당 계산한다.
```

**사용자 쿼리:** "제1조가 뭐야?"

**검색 결과:**
- 모든 "[제1조]" 태그가 매칭됨
- 실제 "제1조" 내용뿐 아니라 태그만 같은 다른 문서도 높은 점수

**벡터 유사도 영향:**
- "[제1조]" 태그 자체가 임베딩 벡터에 영향
- 같은 태그 = 높은 유사도
- 실제 내용보다 태그가 유사도에 기여

#### 검증:
```python
# 임베딩 비교 테스트
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

text1 = "[제1조] 인건비는 월급제를 원칙으로 한다."
text2 = "[제1조] 출장비는 실비정산을 원칙으로 한다."
text3 = "인건비는 월급제를 원칙으로 한다."

vec1 = embeddings.embed_query(text1)
vec2 = embeddings.embed_query(text2)
vec3 = embeddings.embed_query(text3)

# 코사인 유사도 계산
sim_12 = cosine_similarity(vec1, vec2)  # 예상: 0.7 (태그 때문에 높음)
sim_13 = cosine_similarity(vec1, vec3)  # 예상: 0.95 (내용 동일)

# 문제: text1과 text2가 의외로 높은 유사도를 가질 수 있음
```

#### 권장 해결책:

**Option 1: 메타데이터로 분리 (Best Practice)**
```python
def preprocess_text_with_section_headers_v2(text):
    lines = text.split('\n')
    chunks = []
    current_section = "일반"

    for line in lines:
        if header_pattern.match(line):
            current_section = line
        else:
            # 태그를 텍스트에 포함하지 않음
            chunks.append({
                "content": line.strip(),
                "section": current_section  # 메타데이터로 저장
            })

    return chunks

# 색인 시:
docs = [
    Document(
        page_content=chunk["content"],  # 순수 내용만
        metadata={
            "source": fname,
            "section": chunk["section"]  # 섹션은 메타데이터에
        }
    )
    for chunk in chunks
]
```

**Option 2: 태그 가중치 낮추기**
```python
# 임베딩 전에 태그 제거 또는 구분자로 변경
enriched_line = f"{current_section}: {stripped_line}"
# "[제1조]" → "제1조:" 형태로 자연스럽게
```

---

## 🟡 개선 필요 사항 (Medium)

### 5. **중복 제거 로직의 한계**

#### 📍 위치: `app.py:281-290`

```python
all_docs, all_infos, seen = [], [], set()

for q in search_queries:
    docs, infos = search_similar_documents(...)
    for d, i in zip(docs, infos):
        if d.page_content not in seen:  # ⚠️ 완전 동일한 문자열만 제거
            seen.add(d.page_content)
            all_docs.append(d)
```

#### 문제:
- 공백 1개 차이만 있어도 중복으로 인식 안 됨
- 같은 문서의 인접 청크들 (오버랩 200자) 모두 포함

#### 개선안:
```python
import hashlib

def fuzzy_dedup(docs, infos, similarity_threshold=0.9):
    """유사한 청크 제거"""
    filtered = []
    seen_hashes = set()

    for d, i in zip(docs, infos):
        # 정규화: 공백/개행 제거 후 해시
        normalized = re.sub(r'\s+', '', d.page_content)
        content_hash = hashlib.md5(normalized.encode()).hexdigest()

        if content_hash not in seen_hashes:
            seen_hashes.add(content_hash)
            filtered.append((d, i))

    return filtered
```

---

### 6. **메타데이터 추출 에러 처리 미흡**

#### 📍 위치: `app.py:307-308`

```python
source_list = list(set([x[0].metadata.get('source', '문서') for x in combined]))
main_source = "여러 참고 문서" if len(source_list) > 1 else source_list[0]
```

#### 문제:
- `combined`가 비어있지 않은데 구조가 예상과 다를 경우 처리 없음
- 295-301줄에서는 안전 처리했는데 여기는 안 함

#### 권장:
```python
try:
    source_list = list(set([x[0].metadata.get('source', '문서') for x in combined if x and len(x) > 0]))
    main_source = "여러 참고 문서" if len(source_list) > 1 else (source_list[0] if source_list else "문서")
except Exception as e:
    st.warning(f"출처 추출 오류: {e}")
    main_source = "문서"
```

---

### 7. **청크 크기 고정의 비효율성**

#### 📍 위치: `rag_module.py:219`

```python
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
```

#### 문제:
- PDF 표: 1000자로 자르면 의미 손실
- Excel: 행당 50자인데 오버랩 200은 과도
- PowerPoint: 슬라이드별로 이미 구분되어 있는데 재분할

#### 권장:
```python
def get_optimal_splitter(file_type):
    if file_type == 'xlsx':
        return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    elif file_type == 'pptx':
        return RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)
    else:
        return RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
```

---

### 8. **DB 검색 함수의 에러 복구 부재**

#### 📍 위치: `rag_module.py:232-268`

```python
def search_similar_documents(query, client, embeddings, top_k=5):
    try:
        query_vector = embeddings.embed_query(query)  # ⚠️ API 호출 실패 가능
        response = client.rpc("match_documents", params).execute()  # ⚠️ DB 오류 가능
        # ...
    except Exception as e:
        print(f"Search Error: {e}")
        return [], []  # ⚠️ 빈 결과 반환
```

#### 문제:
- 임베딩 API 실패 시 재시도 없음
- DB 연결 오류 시 사용자에게 알림 없음
- 빈 결과 반환 → 사용자는 "문서 없음"으로 오해

#### 권장:
```python
import time

def search_with_retry(query, client, embeddings, top_k=5, max_retries=3):
    for attempt in range(max_retries):
        try:
            return search_similar_documents(query, client, embeddings, top_k)
        except Exception as e:
            if attempt == max_retries - 1:
                st.error(f"검색 실패 ({max_retries}회 시도): {e}")
                return [], []
            time.sleep(1 * (attempt + 1))  # 지수 백오프
```

---

## 📊 진단 체크리스트

### 즉시 실행 가능한 확인 사항:

#### ✅ **1. DB 중복 확인 (최우선)**
```sql
-- Supabase SQL Editor에서 실행
SELECT
    metadata->>'source' as filename,
    COUNT(*) as chunk_count,
    COUNT(DISTINCT DATE(created_at)) as indexed_dates
FROM documents
GROUP BY metadata->>'source'
ORDER BY chunk_count DESC
LIMIT 20;

-- 예상 결과:
-- filename           | chunk_count | indexed_dates
-- 인건비규정.pdf      | 25          | 1  ✅ 정상
-- 출장비규정.pdf      | 48          | 2  ❌ 중복! (2번 색인됨)
```

#### ✅ **2. 검색 점수 분포 확인**
```python
# Streamlit 앱의 관리자 패널에 추가
if st.button("검색 품질 테스트"):
    test_query = st.text_input("테스트 쿼리", "인건비 지급 규정")

    docs, infos = search_similar_documents(
        test_query,
        st.session_state.supabase_client,
        st.session_state.embeddings,
        top_k=20
    )

    st.write("### 검색 결과 점수 분포")
    for i, info in enumerate(infos):
        color = "🟢" if info['score'] > 0.7 else "🟡" if info['score'] > 0.5 else "🔴"
        st.write(f"{color} {i+1}. {info['filename']} - **{info['score']:.3f}**")
```

#### ✅ **3. 쿼리 확장 로그 확인**
```python
# app.py:278 라인에 추가
search_queries = expand_query(query, st.session_state.llm)
st.info(f"🔍 확장된 검색어 ({len(search_queries)}개): {', '.join(search_queries)}")
```

---

## 🎯 결론 및 권장 조치

### 🔴 **즉시 조치 필요 (Critical)**

**1. DB 중복 확인 및 삭제 (1-2시간 소요)**
```bash
# Step 1: SQL로 중복 확인
SELECT metadata->>'source', COUNT(*)
FROM documents
GROUP BY metadata->>'source'
HAVING COUNT(*) > 20;  -- 파일당 20청크 이상이면 의심

# Step 2: 중복 발견 시 → DB 삭제
# Streamlit 관리자 패널 → "DB 삭제" 버튼

# Step 3: 재색인
# "문서 동기화" 버튼 1번만 클릭
```

### 🟠 **단기 개선 (1주일 내)**

**2. 검색 품질 개선**
- 검색 임계값 동적 조정: 0.15 → 0.5 (높음) → 0.15 (낮음) 순차 적용
- 쿼리 확장 완전 단어 매칭으로 변경
- 검색 점수 로그 추가

**3. 중복 방지 메커니즘 추가**
- 파일별 기존 문서 확인 로직
- 파일 해시 기반 중복 체크
- 업데이트 시 기존 삭제 후 재삽입

### 🟡 **중장기 개선 (1개월 내)**

**4. 아키텍처 개선**
- 섹션 태그를 메타데이터로 분리
- 파일별 선택 삭제 기능
- 증분 업데이트 지원

---

## 📌 FAQ

**Q1: DB 삭제 외에 다른 방법은 정말 없나요?**
→ 현재 코드로는 없습니다. 파일별 삭제 함수를 구현하면 가능하지만, 이미 중복이 심각하면 DB 삭제가 더 빠릅니다.

**Q2: DB 삭제 후 다시 중복될 수 있나요?**
→ 네. "문서 동기화"를 여러 번 클릭하면 다시 중복됩니다. 버튼에 경고 메시지 추가 권장.

**Q3: 검색 임계값 0.15를 0.5로 높이면 검색 누락 발생하지 않나요?**
→ 동적 임계값 사용을 권장합니다. 먼저 0.5로 검색 → 결과 부족 시 0.15로 재검색.

**Q4: 쿼리 확장을 완전히 끄면 어떻게 되나요?**
→ 검색 재현율이 낮아집니다. 완전히 끄기보다는 정확도를 높이는 방향 (완전 단어 매칭)을 권장합니다.

---

**보고서 작성:** Claude Code
**분석 기준:** 2025-12-14 코드 스냅샷
